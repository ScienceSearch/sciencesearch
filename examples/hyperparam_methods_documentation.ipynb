{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5307ea-e6cd-4644-99e2-d4f75da338c0",
   "metadata": {},
   "source": [
    "# Methods for Hyperparameterization and Documentation of Model Parameters\n",
    "\n",
    "Learn how this library creates a tuned ensemble algorithm for extracting keywords from text documents. The approach involves optimizing multiple NLP algorithms and combining their best-performing configurations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa5fc9-d3b3-40be-ae27-3358e8d42e6c",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "Import modules and set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636cd436-9882-47f7-9eff-9347b0e51b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sciencesearch.nlp.hyper import Hyper, algorithms_from_results\n",
    "from sciencesearch.nlp.sweep import Sweep\n",
    "from sciencesearch.nlp.models import Rake, Yake, KPMiner, Ensemble\n",
    "from sciencesearch.nlp.train import train_hyper, load_hyper, run_hyper\n",
    "from operator import attrgetter\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "\n",
    "logging.root.setLevel(logging.ERROR)  # silence pke warnings\n",
    "slog = logging.getLogger(\"sciencesearch\")\n",
    "slog.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f981a40c-c8ec-4143-8649-cb85a10f0167",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "3 Step process\n",
    "1. Parameter Tuning\n",
    "\n",
    "2. Evaluate 3 algorithms with a range of settings\n",
    "\n",
    "3. Ensemble approach to unify high-performing algorithms\n",
    "\n",
    "\n",
    "#### Performance Evaluation\n",
    "__F1 Score Metric__\n",
    "\n",
    "The primary evaluation metric is F1 scoring which balances two key performance aspects, precision and recall, and is the industry standard. \n",
    "\n",
    "***\n",
    "\n",
    "## Outcome \n",
    "An ensemble algorithm that provides optimized keyword extraction, both combining the strengths of multiple algorithms and maintaining high precision and recall performance.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59525438",
   "metadata": {},
   "source": [
    "#### Step 1: Parameter Tuning Process\n",
    "\n",
    "We tune algorithm parameters specifically for our target text type to create an effective keyword extraction system. \n",
    "\n",
    "This process:\n",
    "\n",
    "1. Establish Ground Truth: Provide \"gold standard\" keywords for sample documents\n",
    "\n",
    "2. Explore Parameters: Test various parameter combinations across multiple algorithms\n",
    "\n",
    "3. Evaluate Performance: Compare automated results against the gold standard\n",
    "\n",
    "4. Determine Selection Criteria: Identify configurations that achieve near-optimal performance\n",
    "\n",
    "\n",
    "*Note: The F1 score balances two performance metrics: precision and recall. In terms of this case, precision is the proportion of keywords generated that match the gold standard, and recall is the proportion of the gold standard keywords that were generated at all. Since these two metrics tend to vary inversely (in particular, generating _lots_ of keywords tends to give good recall but poor precision) the F1 tries to balance them by taking their harmonic mean. The result is that, roughly speaking, the F1 reflects the lower of the two scores.*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018e92c",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Joint Multi-Algorithm Approach\n",
    "\n",
    "__We test three complementary NLP algorithms__\n",
    "\n",
    "1. RAKE (Rapid Automatic Keyword Extraction)\n",
    "   \n",
    "2. YAKE (Yet Another Keyword Extractor)\n",
    "    \n",
    "3. KPMiner (Keyphrase Mining algorithm)\n",
    "\n",
    "Each algorithm is tested across multiple parameter settings to find optimal configurations.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d3014d",
   "metadata": {},
   "source": [
    "#### Step 3: Ensemble Creation\n",
    "\n",
    "The final system combines multiple high-performing algorithm/parameter combinations into a unified ensemble that:\n",
    "\n",
    "1. Takes the union of keywords from each component algorithm\n",
    "\n",
    "2. Leverages the strengths of different extraction approaches\n",
    "\n",
    "3. Provides more robust and comprehensive keyword identification\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4625bc",
   "metadata": {},
   "source": [
    "## Walk through of Hyperparameter Optemization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c2903a3-5568-4685-a3e6-deffd8019fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hyperparameter object\n",
    "hyperparameter = Hyper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c07cc2-ddc4-4b31-b9c8-d5a5e1f63be7",
   "metadata": {},
   "source": [
    "### Set up parameter sweeps\n",
    "The `Sweep` class from the `sciencesearch.nlp.sweep` module is used to configure the algorithm and range of parameters to use in the hyperparameter tuning.\n",
    "\n",
    "\n",
    "The list of possible parameters is shown with the `.print_params` method of each algorithm class. \n",
    "*Note that these include a set of parameters shared across all the algorithms, for which there are reasonable defaults.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e991ec2f-9b2d-471a-971c-72d989bd6fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 7\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "Yake:\n",
      "  - int ws: YAKE window size. Default is 2\n",
      "  - float dedup: Deduplication limit for YAKE. Default is 0.9\n",
      "  - str dedup_method: method ('leve', 'seqm' or 'jaro'). Default is leve\n",
      "  - int ngram: Maximum ngram size. Default is 2\n"
     ]
    }
   ],
   "source": [
    "Yake.print_params()\n",
    "sweep = Sweep(alg=Yake)\n",
    "sweep.set_param_range(\"ws\", lb=1, ub=3, step=1)\n",
    "sweep.set_param_discrete(\"dedup\", [0.8, 0.9, 0.95])\n",
    "sweep.set_param_discrete(\"dedup_method\", [\"leve\", \"seqm\"])  # jaro\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e25a39-a6e7-47dd-86ba-90782ac3d3ad",
   "metadata": {},
   "source": [
    "Common:\n",
    "  - Stopwords stopwords: Stopwords. Default is None\n",
    "  - bool stemming: Whether to do stemming. Default is False\n",
    "  - int num_keywords: How many keywords to extract. Default is 10\n",
    "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
    "Yake:\n",
    "  - int ws: YAKE window size. Default is 2\n",
    "  - float dedup: Deduplication limit for YAKE. Default is 0.9\n",
    "  - str dedup_method: method ('leve', 'seqm' or 'jaro'). Default is leve\n",
    "  - int ngram: Maximum ngram size. Default is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a44897-58f8-4012-8df1-44184e0dcaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 7\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "Rake:\n",
      "  - int min_len: Minimum ngram size. Default is 1\n",
      "  - int max_len: Maximum ngram size. Default is 3\n",
      "  - int min_kw_len: Minimum keyword length. Applied as post-processing filter.. Default is 3\n",
      "  - int min_kw_occ: Mimumum number of occurences of keyword in text string.Applied as post-processing filter.. Default is 4\n",
      "  - Any ranking_metric: ranking parameter for rake algorithm. Default is Metric.DEGREE_TO_FREQUENCY_RATIO\n",
      "  - bool include_repeated_phrases: boolean for determining whether multiple of the same keywords are output by rake. Default is True\n"
     ]
    }
   ],
   "source": [
    "Rake.print_params()\n",
    "sweep = Sweep(alg=Rake)\n",
    "sweep.set_param_range(\"min_len\", lb=1, ub=1, step=1)\n",
    "sweep.set_param_range(\"max_len\", lb=1, ub=3, step=1)\n",
    "sweep.set_param_range(\"min_kw_occ\", lb=1, ub=10, step=1)\n",
    "sweep.set_param_discrete(\"include_repeated_phrases\", [False, True])\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b8e8f-a876-473c-bfdb-91bd6cbb3693",
   "metadata": {},
   "source": [
    "Common:\n",
    "  - Stopwords stopwords: Stopwords. Default is None\n",
    "  - bool stemming: Whether to do stemming. Default is False\n",
    "  - int num_keywords: How many keywords to extract. Default is 10\n",
    "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
    "Rake:\n",
    "  - int min_len: Minimum ngram size. Default is 1\n",
    "  - int max_len: Maximum ngram size. Default is 3\n",
    "  - int min_kw_len: Minimum keyword length. Applied as post-processing filter.. Default is 3\n",
    "  - int min_kw_occ: Mimumum number of occurences of keyword in text string.Applied as post-processing filter.. Default is 4\n",
    "  - Any ranking_metric: ranking parameter for rake algorithm. Default is Metric.DEGREE_TO_FREQUENCY_RATIO\n",
    "  - bool include_repeated_phrases: boolean for determining whether multiple of the same keywords are output by rake. Default is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7eaa958-c7dc-486b-824a-5f36a25e92cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 7\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "KPMiner:\n",
      "  - int lasf: Last allowable seen frequency. Default is 3\n",
      "  - int cutoff: Cutoff threshold for number of words after which if a phrase appears for the first time it is ignored. Default is 400\n",
      "  - float alpha: Weight-adjustment parameter 1 for boosting factor.See original paper for definition. Default is 2.3\n",
      "  - float sigma: Weight-adjustment parameter 2 for boosting factor.See original paper for definition. Default is 3.0\n",
      "  - object doc_freq_info: Document frequency counts. Default (None) uses the semeval2010 countsprovided in 'df-semeval2010.tsv.gz'. Default is None\n"
     ]
    }
   ],
   "source": [
    "KPMiner.print_params()\n",
    "sweep = Sweep(alg=KPMiner)\n",
    "sweep.set_param_range(\"lasf\", lb=1, ub=3, step=1)\n",
    "# commenting out because ..zong...this takes forever..\n",
    "# sweep.set_param_range(\"cutoff\", lb=200, ub=1300, nsteps=5)\n",
    "# sweep.set_param_range(\"alpha\", lb=3.0, ub=4.0, step=0.2)\n",
    "# sweep.set_param_range(\"sigma\", lb=2.6, ub=3.2, step=0.2)\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0df6b4-d4b7-4869-b005-878801511b7e",
   "metadata": {},
   "source": [
    "Common:\n",
    "  - Stopwords stopwords: Stopwords. Default is None\n",
    "  - bool stemming: Whether to do stemming. Default is False\n",
    "  - int num_keywords: How many keywords to extract. Default is 10\n",
    "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
    "KPMiner:\n",
    "  - int lasf: Last allowable seen frequency. Default is 3\n",
    "  - int cutoff: Cutoff threshold for number of words after which if a phrase appears for the first time it is ignored. Default is 400\n",
    "  - float alpha: Weight-adjustment parameter 1 for boosting factor.See original paper for definition. Default is 2.3\n",
    "  - float sigma: Weight-adjustment parameter 2 for boosting factor.See original paper for definition. Default is 3.0\n",
    "  - object doc_freq_info: Document frequency counts. Default (None) uses the semeval2010 countsprovided in 'df-semeval2010.tsv.gz'. Default is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ce78f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
