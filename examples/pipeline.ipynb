{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5307ea-e6cd-4644-99e2-d4f75da338c0",
   "metadata": {},
   "source": [
    "# ScienceSearch NLP Keywords Example\n",
    "How to use the natural language processing (NLP) to generate high-quality metadata (keywords) for searching documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa5fc9-d3b3-40be-ae27-3358e8d42e6c",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "Import modules and set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636cd436-9882-47f7-9eff-9347b0e51b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sciencesearch.nlp.hyper import Hyper, algorithms_from_results\n",
    "from sciencesearch.nlp.sweep import Sweep\n",
    "from sciencesearch.nlp.models import Rake, Yake, KPMiner, Ensemble\n",
    "from sciencesearch.nlp.train import train_hyper, load_hyper, run_hyper\n",
    "from operator import attrgetter\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "\n",
    "logging.root.setLevel(logging.ERROR)  # silence pke warnings\n",
    "slog = logging.getLogger(\"sciencesearch\")\n",
    "slog.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307807c8-932a-4d6f-8871-ae18c28a0e7c",
   "metadata": {},
   "source": [
    "## Generate a tuned algorithm for extracting keywords\n",
    "First step is to tune the parameters of the available algorithms to the particular type of text that will be processed. This is best done by providing some \"gold standard\" keywords for sample documents, then allowing the system to run combinations of parameters to experimentally see which comes closest to generating the same keywords automatically. Our approach here will be to run 3 different NLP algorithms -- Rake, Yake, and KPMiner -- across a variety of settings, and pick all combinations that come close to the \"best\" F1 score. These algorithm/parameter combinations will be encapsulated in an \"ensemble\" algorithm that will take the union of the keywords generated by each individual algorithm.\n",
    "\n",
    "Note: The F1 score balances two performance metrics: precision and recall. In terms of this case, precision is the proportion of keywords generated that match the gold standard, and recall is the proportion of the gold standard keywords that were generated at all. Since these two metrics tend to vary inversely (in particular, generating _lots_ of keywords tends to give good recall but poor precision) the F1 tries to balance them by taking their harmonic mean. The result is that, roughly speaking, the F1 reflects the lower of the two scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb9bfb6-cc9c-495f-8a1a-eb603993443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdir = Path.cwd().parent / \"data\" / \"jft\"\n",
    "\n",
    "epsilon = 0.1\n",
    "max_alg = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f05fb178-d444-48e9-a123-4be10af07ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = Hyper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578551b7-5395-41c7-983f-77c07ac76c22",
   "metadata": {},
   "source": [
    "## Set up parameter sweeps\n",
    "The `Sweep` class from the `sciencesearch.nlp.sweep` module is used to configure the algorithm and range of parameters to use in the hyperparameter tuning.\n",
    "The list of possible parameters is shown with the `.print_params` method of each algorithm class. Note that these include a set of parameters shared across all the algorithms, for which there are reasonable defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea1b2512-fb07-42cf-a810-f017170a5f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 10\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "Yake:\n",
      "  - int ws: YAKE window size. Default is 2\n",
      "  - float dedup: Deduplication limit for YAKE. Default is 0.9\n",
      "  - str dedup_method: method ('leve', 'seqm' or 'jaro'). Default is leve\n",
      "  - int ngram: Maximum ngram size. Default is 2\n"
     ]
    }
   ],
   "source": [
    "Yake.print_params()\n",
    "sweep = Sweep(alg=Yake)\n",
    "sweep.set_param_range(\"ws\", lb=1, ub=3, step=1)\n",
    "sweep.set_param_discrete(\"dedup\", [0.8, 0.9, 0.95])\n",
    "sweep.set_param_discrete(\"dedup_method\", [\"leve\", \"seqm\"])  # jaro\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd3387f-2a0c-4699-b006-8e9f7ec7140b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 10\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "Rake:\n",
      "  - int min_len: Minimum ngram size. Default is 1\n",
      "  - int max_len: Maximum ngram size. Default is 3\n",
      "  - int min_kw_len: Minimum keyword length. Applied as post-processing filter.. Default is 3\n",
      "  - int min_kw_occ: Mimumum number of occurences of keyword in text string.Applied as post-processing filter.. Default is 4\n",
      "  - Any ranking_metric: ranking parameter for rake algorithm. Default is Metric.DEGREE_TO_FREQUENCY_RATIO\n",
      "  - bool include_repeated_phrases: boolean for determining whether multiple of the same keywords are output by rake. Default is True\n"
     ]
    }
   ],
   "source": [
    "Rake.print_params()\n",
    "sweep = Sweep(alg=Rake)\n",
    "sweep.set_param_range(\"min_len\", lb=1, ub=1, step=1)\n",
    "sweep.set_param_range(\"max_len\", lb=1, ub=3, step=1)\n",
    "sweep.set_param_range(\"min_kw_occ\", lb=1, ub=10, step=1)\n",
    "sweep.set_param_discrete(\"include_repeated_phrases\", [False, True])\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2244c501-5dee-4ffa-a2bc-6d110d74398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 10\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "KPMiner:\n",
      "  - int lasf: Last allowable seen frequency. Default is 3\n",
      "  - int cutoff: Cutoff threshold for number of words after which if a phrase appears for the first time it is ignored. Default is 400\n",
      "  - float alpha: Weight-adjustment parameter 1 for boosting factor.See original paper for definition. Default is 2.3\n",
      "  - float sigma: Weight-adjustment parameter 2 for boosting factor.See original paper for definition. Default is 3.0\n",
      "  - object doc_freq_info: Document frequency counts. Default (None) uses the semeval2010 countsprovided in 'df-semeval2010.tsv.gz'. Default is None\n"
     ]
    }
   ],
   "source": [
    "KPMiner.print_params()\n",
    "sweep = Sweep(alg=KPMiner)\n",
    "sweep.set_param_range(\"lasf\", lb=1, ub=3, step=1)\n",
    "# zomg this takes forever..\n",
    "# sweep.set_param_range(\"cutoff\", lb=200, ub=1300, nsteps=5)\n",
    "# sweep.set_param_range(\"alpha\", lb=3.0, ub=4.0, step=0.2)\n",
    "# sweep.set_param_range(\"sigma\", lb=2.6, ub=3.2, step=0.2)\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0f4f6-2f39-4236-b2e7-c44e1f84dbd4",
   "metadata": {},
   "source": [
    "## Train and run models\n",
    "In this example, we pick the 'best' result for each algorithm by training on two files with some user-provided keywords.\n",
    "Then we extract keywords from a third file using the trained model.\n",
    "\n",
    "We save the results of the hyperparameter training in a serialize Python \"pickle\" file so we don't need to repeat the training.\n",
    "We could run the same hyperparameters on multiple files without retraining with `run_hyper()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a54884-5ed3-49a4-be39-b5f3c610d192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords for 'momotaro':\n",
      "boy baby\n",
      "treasure\n",
      "hidden treasure\n",
      "Ogres’\n",
      "washing\n",
      "woman\n",
      "taste\n",
      "adventures\n",
      "Ogres’ Island\n",
      "Lord Momotaro\n",
      "wow\n",
      "worth\n",
      "woman lived\n",
      "wallet\n",
      "peach\n",
      "good\n",
      "Ken\n",
      "Momotaro gave\n",
      "time\n",
      "tortoise\n",
      "beasts talked\n",
      "tied\n",
      "good man\n",
      "Momotaro\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "file_kw = {\n",
    "    \"the_flute\": [\n",
    "        \"flute\",\n",
    "        \"bamboo\",\n",
    "        \"jealous\",\n",
    "        \"Kioto\",\n",
    "        \"O’Yoné\",\n",
    "        \"father\",\n",
    "        \"stepmother\",\n",
    "    ],\n",
    "    \"the_good_thunder\": [\n",
    "        \"Rai-den\",\n",
    "        \"Thunder\",\n",
    "        \"Rai-Taro\",\n",
    "        \"cloud\",\n",
    "        \"lightning\",\n",
    "        \"Lady Kwannon\",\n",
    "        \"white cloud\",\n",
    "    ],\n",
    "}\n",
    "pickle_file = Path(\"hyper.pkl\")\n",
    "if pickle_file.exists():\n",
    "    hres = load_hyper(pickle_file)\n",
    "else:\n",
    "    hres = train_hyper(hyperparameter, file_kw, epsilon, \"hyper.pkl\", directory=textdir)\n",
    "keywords = run_hyper(hres, text_file=textdir / \"momotaro\")\n",
    "print(\"Keywords for 'momotaro':\")\n",
    "print(\"\\n\".join(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73071707-f530-4bbe-97dd-3df3a47c5835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
