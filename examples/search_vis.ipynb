{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4ac1bb",
   "metadata": {},
   "source": [
    "# ScienceSearch NLP Keywords with Visualiztion and Saving Results Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a93e9dd",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "Import modules and set up logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386f073",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341a62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "from sciencesearch.nlp.hyper import Hyper, algorithms_from_results\n",
    "from sciencesearch.nlp.sweep import Sweep\n",
    "from sciencesearch.nlp.models import Rake, Yake, KPMiner, Ensemble\n",
    "from sciencesearch.nlp.train import train_hyper, load_hyper, run_hyper\n",
    "from sciencesearch.nlp.search import Searcher\n",
    "from operator import attrgetter\n",
    "# logging\n",
    "import logging\n",
    "logging.root.setLevel(logging.ERROR)  # silence pke warnings\n",
    "slog = logging.getLogger(\"sciencesearch\")\n",
    "slog.setLevel(logging.WARNING)\n",
    "from sciencesearch.nlp.visualize_kws import JsonView\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e3a8b",
   "metadata": {},
   "source": [
    "## Generate a tuned algorithm for extracting keywords\n",
    "First step is to tune the parameters of the available algorithms to the particular type of text that will be processed. This is best done by providing some \"gold standard\" keywords for sample documents, then allowing the system to run combinations of parameters to experimentally see which comes closest to generating the same keywords automatically. Our approach here will be to run 3 different NLP algorithms -- Rake, Yake, and KPMiner -- across a variety of settings, and pick all combinations that come close to the \"best\" F1 score. These algorithm/parameter combinations will be encapsulated in an \"ensemble\" algorithm that will take the union of the keywords generated by each individual algorithm.\n",
    "\n",
    "Note: The F1 score balances two performance metrics: precision and recall. In terms of this case, precision is the proportion of keywords generated that match the gold standard, and recall is the proportion of the gold standard keywords that were generated at all. Since these two metrics tend to vary inversely (in particular, generating _lots_ of keywords tends to give good recall but poor precision) the F1 tries to balance them by taking their harmonic mean. The result is that, roughly speaking, the F1 reflects the lower of the two scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba1129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdir = Path.cwd().parent / \"data\" / \"jft\"\n",
    "\n",
    "epsilon = 0.1\n",
    "max_alg = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9befdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = Hyper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb7670",
   "metadata": {},
   "source": [
    "## Set up parameter sweeps\n",
    "The `Sweep` class from the `sciencesearch.nlp.sweep` module is used to configure the algorithm and range of parameters to use in the hyperparameter tuning.\n",
    "The list of possible parameters is shown with the `.print_params` method of each algorithm class. Note that these include a set of parameters shared across all the algorithms, for which there are reasonable defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb12ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 10\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "Yake:\n",
      "  - int ws: YAKE window size. Default is 2\n",
      "  - float dedup: Deduplication limit for YAKE. Default is 0.9\n",
      "  - str dedup_method: method ('leve', 'seqm' or 'jaro'). Default is leve\n",
      "  - int ngram: Maximum ngram size. Default is 2\n"
     ]
    }
   ],
   "source": [
    "Yake.print_params()\n",
    "sweep = Sweep(alg=Yake)\n",
    "sweep.set_param_range(\"ws\", lb=1, ub=3, step=1)\n",
    "sweep.set_param_discrete(\"dedup\", [0.8, 0.9, 0.95])\n",
    "sweep.set_param_discrete(\"dedup_method\", [\"leve\", \"seqm\"]) # jaro\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c7634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 10\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "Rake:\n",
      "  - int min_len: Minimum ngram size. Default is 1\n",
      "  - int max_len: Maximum ngram size. Default is 3\n",
      "  - int min_kw_len: Minimum keyword length. Applied as post-processing filter.. Default is 3\n",
      "  - int min_kw_occ: Mimumum number of occurences of keyword in text string.Applied as post-processing filter.. Default is 4\n",
      "  - Any ranking_metric: ranking parameter for rake algorithm. Default is Metric.DEGREE_TO_FREQUENCY_RATIO\n",
      "  - bool include_repeated_phrases: boolean for determining whether multiple of the same keywords are output by rake. Default is True\n"
     ]
    }
   ],
   "source": [
    "Rake.print_params()\n",
    "sweep = Sweep(alg=Rake)\n",
    "sweep.set_param_range(\"min_len\", lb=1, ub=1, step=1)\n",
    "sweep.set_param_range(\"max_len\", lb=1, ub=3, step=1)\n",
    "sweep.set_param_range(\"min_kw_occ\", lb=1, ub=10, step=1)\n",
    "sweep.set_param_discrete(\"include_repeated_phrases\", [False, True])\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b5898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 10\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "KPMiner:\n",
      "  - int lasf: Last allowable seen frequency. Default is 3\n",
      "  - int cutoff: Cutoff threshold for number of words after which if a phrase appears for the first time it is ignored. Default is 400\n",
      "  - float alpha: Weight-adjustment parameter 1 for boosting factor.See original paper for definition. Default is 2.3\n",
      "  - float sigma: Weight-adjustment parameter 2 for boosting factor.See original paper for definition. Default is 3.0\n",
      "  - object doc_freq_info: Document frequency counts. Default (None) uses the semeval2010 countsprovided in 'df-semeval2010.tsv.gz'. Default is None\n"
     ]
    }
   ],
   "source": [
    "KPMiner.print_params()\n",
    "sweep = Sweep(alg=KPMiner)\n",
    "sweep.set_param_range(\"lasf\", lb=1, ub=3, step=1)\n",
    "# zomg this takes forever..\n",
    "#sweep.set_param_range(\"cutoff\", lb=200, ub=1300, nsteps=5)\n",
    "#sweep.set_param_range(\"alpha\", lb=3.0, ub=4.0, step=0.2)\n",
    "#sweep.set_param_range(\"sigma\", lb=2.6, ub=3.2, step=0.2)\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2d00c",
   "metadata": {},
   "source": [
    "## Train and run models\n",
    "In this example, we pick the 'best' result for each algorithm by training on two files with some user-provided keywords.\n",
    "Then we extract keywords from a third file using the trained model.\n",
    "\n",
    "Using a searcher which will read in training data from a search configuration, select the best model's keywords. \n",
    "We save the results of the hyperparameter training in a serialize Python \"pickle\" file so we don't need to repeat the training.\n",
    "We could run the same hyperparameters on multiple files without retraining with `run_hyper()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf3906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Searcher()\n",
    "demo = Searcher.from_config(\"seach_vis_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612cf91",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "In this example, we will save the results for easy viewing. First, as a json, then in a color coded html page that allows for easy viewing of the keywords in context. \n",
    "\n",
    "There are two options for saving and visualizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32627539",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_viewer = JsonView(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c204063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1.txt': ['hunter',\n",
       "  'princes',\n",
       "  'Mystic Mountain',\n",
       "  'mimulus',\n",
       "  'Fugi',\n",
       "  'dreamer'],\n",
       " 'file2.txt': ['Aya',\n",
       "  'daimyo',\n",
       "  'Lady Aya',\n",
       "  'moon',\n",
       "  'maidens',\n",
       "  'garden',\n",
       "  'Lord',\n",
       "  'Lord of Ako']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.training_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "096c634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method JsonView.save_predicted_keywords of <sciencesearch.nlp.visualize_kws.JsonView object at 0x342ac0040>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_viewer.save_predicted_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a7ea2",
   "metadata": {},
   "source": [
    "First, only one set of keywords. \n",
    "\n",
    "In this example, predicted keywords are saved, and the resulting saved json is visualized as a html page per input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d2c1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_viewer.save_predicted_keywords(filename = '../examples/results/predicted_keywords.json')\n",
    "JsonView.visualize_from_config(config_file=\"seach_vis_config.json\", json_file=\"../examples/results/predicted_keywords.json\", save_filename=\"../examples/results/predicted_keywords_html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a5209",
   "metadata": {},
   "source": [
    "Second, multiple sets ofkeywords. \n",
    "\n",
    "In this example, predicted keywords and training keywords are saved, and the resulting saved json is visualized as a html page per input file, color coded to differentiate each set of keywords with a key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f721d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_viewer.save_all_keyword_sets('../examples/results/keywords_all_sets.json')\n",
    "JsonView.visualize_from_config(config_file=\"seach_vis_config.json\",json_file=\"../examples/results/keywords_all_sets.json\", save_filename=\"../examples/results/keywords_all_sets_html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
