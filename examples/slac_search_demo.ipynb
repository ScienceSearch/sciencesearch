{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191c5016",
   "metadata": {},
   "source": [
    "# ScienceSearch NLP Keywords with Visualiztion and Saving Results Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470daf6",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "Import modules and set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82af33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "from sciencesearch.nlp.hyper import Hyper, algorithms_from_results\n",
    "from sciencesearch.nlp.sweep import Sweep\n",
    "from sciencesearch.nlp.models import Rake, Yake, KPMiner, Ensemble\n",
    "from sciencesearch.nlp.train import train_hyper, load_hyper, run_hyper\n",
    "from sciencesearch.nlp.search import Searcher\n",
    "from operator import attrgetter\n",
    "# logging\n",
    "import logging\n",
    "logging.root.setLevel(logging.ERROR)  # silence pke warnings\n",
    "slog = logging.getLogger(\"sciencesearch\")\n",
    "slog.setLevel(logging.WARNING)\n",
    "from sciencesearch.nlp.visualize_kws import JsonView\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff369f4",
   "metadata": {},
   "source": [
    "## Generate a tuned algorithm for extracting keywords\n",
    "First step is to tune the parameters of the available algorithms to the particular type of text that will be processed. This is best done by providing some \"gold standard\" keywords for sample documents, then allowing the system to run combinations of parameters to experimentally see which comes closest to generating the same keywords automatically. Our approach here will be to run 3 different NLP algorithms -- Rake, Yake, and KPMiner -- across a variety of settings, and pick all combinations that come close to the \"best\" F1 score. These algorithm/parameter combinations will be encapsulated in an \"ensemble\" algorithm that will take the union of the keywords generated by each individual algorithm.\n",
    "\n",
    "Note: The F1 score balances two performance metrics: precision and recall. In terms of this case, precision is the proportion of keywords generated that match the gold standard, and recall is the proportion of the gold standard keywords that were generated at all. Since these two metrics tend to vary inversely (in particular, generating _lots_ of keywords tends to give good recall but poor precision) the F1 tries to balance them by taking their harmonic mean. The result is that, roughly speaking, the F1 reflects the lower of the two scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082857ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = Hyper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97c562",
   "metadata": {},
   "source": [
    "## Set up parameter sweeps\n",
    "The `Sweep` class from the `sciencesearch.nlp.sweep` module is used to configure the algorithm and range of parameters to use in the hyperparameter tuning.\n",
    "The list of possible parameters is shown with the `.print_params` method of each algorithm class. Note that these include a set of parameters shared across all the algorithms, for which there are reasonable defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d48b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 10\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "Yake:\n",
      "  - int ws: YAKE window size. Default is 2\n",
      "  - float dedup: Deduplication limit for YAKE. Default is 0.9\n",
      "  - str dedup_method: method ('leve', 'seqm' or 'jaro'). Default is leve\n",
      "  - int ngram: Maximum ngram size. Default is 2\n"
     ]
    }
   ],
   "source": [
    "Yake.print_params()\n",
    "sweep = Sweep(alg=Yake)\n",
    "sweep.set_param_range(\"ws\", lb=1, ub=3, step=1)\n",
    "sweep.set_param_discrete(\"dedup\", [0.8, 0.9, 0.95])\n",
    "sweep.set_param_discrete(\"dedup_method\", [\"leve\", \"seqm\"]) # jaro\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34c3fd",
   "metadata": {},
   "source": [
    "Common:\n",
    "  - Stopwords stopwords: Stopwords. Default is None\n",
    "  - bool stemming: Whether to do stemming. Default is False\n",
    "  - int num_keywords: How many keywords to extract. Default is 10\n",
    "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
    "Yake:\n",
    "  - int ws: YAKE window size. Default is 2\n",
    "  - float dedup: Deduplication limit for YAKE. Default is 0.9\n",
    "  - str dedup_method: method ('leve', 'seqm' or 'jaro'). Default is leve\n",
    "  - int ngram: Maximum ngram size. Default is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5596ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 10\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "Rake:\n",
      "  - int min_len: Minimum ngram size. Default is 1\n",
      "  - int max_len: Maximum ngram size. Default is 3\n",
      "  - int min_kw_len: Minimum keyword length. Applied as post-processing filter.. Default is 3\n",
      "  - int min_kw_occ: Mimumum number of occurences of keyword in text string.Applied as post-processing filter.. Default is 4\n",
      "  - Any ranking_metric: ranking parameter for rake algorithm. Default is Metric.DEGREE_TO_FREQUENCY_RATIO\n",
      "  - bool include_repeated_phrases: boolean for determining whether multiple of the same keywords are output by rake. Default is True\n"
     ]
    }
   ],
   "source": [
    "Rake.print_params()\n",
    "sweep = Sweep(alg=Rake)\n",
    "sweep.set_param_range(\"min_len\", lb=1, ub=1, step=1)\n",
    "sweep.set_param_range(\"max_len\", lb=1, ub=3, step=1)\n",
    "sweep.set_param_range(\"min_kw_occ\", lb=1, ub=10, step=1)\n",
    "sweep.set_param_discrete(\"include_repeated_phrases\", [False, True])\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04fc1c",
   "metadata": {},
   "source": [
    "Common:\n",
    "  - Stopwords stopwords: Stopwords. Default is None\n",
    "  - bool stemming: Whether to do stemming. Default is False\n",
    "  - int num_keywords: How many keywords to extract. Default is 10\n",
    "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
    "Rake:\n",
    "  - int min_len: Minimum ngram size. Default is 1\n",
    "  - int max_len: Maximum ngram size. Default is 3\n",
    "  - int min_kw_len: Minimum keyword length. Applied as post-processing filter.. Default is 3\n",
    "  - int min_kw_occ: Mimumum number of occurences of keyword in text string.Applied as post-processing filter.. Default is 4\n",
    "  - Any ranking_metric: ranking parameter for rake algorithm. Default is Metric.DEGREE_TO_FREQUENCY_RATIO\n",
    "  - bool include_repeated_phrases: boolean for determining whether multiple of the same keywords are output by rake. Default is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df79bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common:\n",
      "  - Stopwords stopwords: Stopwords. Default is None\n",
      "  - bool stemming: Whether to do stemming. Default is False\n",
      "  - int num_keywords: How many keywords to extract. Default is 10\n",
      "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
      "KPMiner:\n",
      "  - int lasf: Last allowable seen frequency. Default is 3\n",
      "  - int cutoff: Cutoff threshold for number of words after which if a phrase appears for the first time it is ignored. Default is 400\n",
      "  - float alpha: Weight-adjustment parameter 1 for boosting factor.See original paper for definition. Default is 2.3\n",
      "  - float sigma: Weight-adjustment parameter 2 for boosting factor.See original paper for definition. Default is 3.0\n",
      "  - object doc_freq_info: Document frequency counts. Default (None) uses the semeval2010 countsprovided in 'df-semeval2010.tsv.gz'. Default is None\n"
     ]
    }
   ],
   "source": [
    "KPMiner.print_params()\n",
    "sweep = Sweep(alg=KPMiner)\n",
    "sweep.set_param_range(\"lasf\", lb=1, ub=3, step=1)\n",
    "# zomg this takes forever..\n",
    "#sweep.set_param_range(\"cutoff\", lb=200, ub=1300, nsteps=5)\n",
    "#sweep.set_param_range(\"alpha\", lb=3.0, ub=4.0, step=0.2)\n",
    "#sweep.set_param_range(\"sigma\", lb=2.6, ub=3.2, step=0.2)\n",
    "hyperparameter.add_sweep(sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b68183",
   "metadata": {},
   "source": [
    "Common:\n",
    "  - Stopwords stopwords: Stopwords. Default is None\n",
    "  - bool stemming: Whether to do stemming. Default is False\n",
    "  - int num_keywords: How many keywords to extract. Default is 10\n",
    "  - list keyword_sort: sort orderings: occ (number of occurrences), score, or a dict with weights for each of these keys, e.g., {'occ': 0.75, 'score': 0.25}, and additionally a flag 'i' for ignoring keyword case. Default is []\n",
    "KPMiner:\n",
    "  - int lasf: Last allowable seen frequency. Default is 3\n",
    "  - int cutoff: Cutoff threshold for number of words after which if a phrase appears for the first time it is ignored. Default is 400\n",
    "  - float alpha: Weight-adjustment parameter 1 for boosting factor.See original paper for definition. Default is 2.3\n",
    "  - float sigma: Weight-adjustment parameter 2 for boosting factor.See original paper for definition. Default is 3.0\n",
    "  - object doc_freq_info: Document frequency counts. Default (None) uses the semeval2010 countsprovided in 'df-semeval2010.tsv.gz'. Default is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db528d24",
   "metadata": {},
   "source": [
    "### Create configuration to automatically train and build a Searcher object that allows you to find files by their predicted and gold keywords\n",
    "\n",
    "See example:  examples/search-vis-demo-config\n",
    "\n",
    "To use existing configuration:\n",
    "1. Add input files to `private_data/slac_logs'\n",
    "2. Add training keyword file to slac_keywords.csv\n",
    "    - the format is `filename`, `\"list, of, keywords, as, a, comma, separated, string\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f9457",
   "metadata": {},
   "source": [
    "#### Understanding the config file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239fab6",
   "metadata": {},
   "source": [
    "\n",
    "##### Section 1: should include algorithms: Yake, Rake, and/or KPMiner\n",
    "\n",
    "In `algorithms` include:\n",
    "- `yake`\n",
    "    - `module`: `sciencesearch.nlp.models`\n",
    "    - `class`: `Yake`\n",
    "- `rake`\n",
    "    - `module`: `sciencesearch.nlp.models`\n",
    "    - `class`: `Rake`\n",
    "- `kpminer`\n",
    "    - `module`: `sciencesearch.nlp.models`\n",
    "    - `class`: `KPMiner`\n",
    "\n",
    "<details> <summary>Example</summary>\n",
    "  \n",
    "```json\n",
    "\"algorithms\": {\n",
    "        \"yake\": {\n",
    "            \"module\": \"sciencesearch.nlp.models\",\n",
    "            \"class\": \"Yake\"\n",
    "        },\n",
    "        \"rake\": {\n",
    "            \"module\": \"sciencesearch.nlp.models\",\n",
    "            \"class\": \"Rake\"\n",
    "        },\n",
    "        \"kpminer\": {\n",
    "            \"module\": \"sciencesearch.nlp.models\",\n",
    "            \"class\": \"KPMiner\"\n",
    "        }\n",
    "    },\n",
    "    ```\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681ccc8",
   "metadata": {},
   "source": [
    "##### Section 2: Define potential hyperparameter combinations\n",
    "\n",
    "See `Set up parameter sweeps` for details\n",
    "\n",
    "<details> <summary>Example</summary>\n",
    "  \n",
    "``` json\n",
    "\"sweeps\": {\n",
    "        \"kpminer\": {\n",
    "            \"lasf\": {\n",
    "                \"_type\": \"range\",\n",
    "                \"lb\": 1,\n",
    "                \"ub\": 3,\n",
    "                \"step\": 1\n",
    "            },\n",
    "            \"-cutoff\": {\n",
    "                \"_type\": \"range\",\n",
    "                \"lb\": 200,\n",
    "                \"ub\": 1300,\n",
    "                \"nsteps\": 5\n",
    "            },\n",
    "            \"-alpha\": {\n",
    "                \"_type\": \"range\",\n",
    "                \"lb\": 3.0,\n",
    "                \"ub\": 4.0,\n",
    "                \"step\": 0.2\n",
    "            },\n",
    "            \"sigma\": {\n",
    "                \"_type\": \"range\",\n",
    "                \"lb\": 2.8,\n",
    "                \"ub\": 3.0,\n",
    "                \"step\": 0.2\n",
    "            }\n",
    "        }\n",
    "        \"rake\": {\n",
    "            \"min_len\": {\n",
    "                \"_type\": \"range\",\n",
    "                \"lb\": 1,\n",
    "                \"ub\": 1,\n",
    "                \"step\": 1\n",
    "            },\n",
    "            \"max_len\": {\n",
    "                \"_type\": \"range\",\n",
    "                \"lb\": 1,\n",
    "                \"ub\": 3,\n",
    "                \"step\": 1\n",
    "            },\n",
    "            \"min_kw_occ\": {\n",
    "                \"_type\": \"range\",\n",
    "                \"lb\": 1,\n",
    "                \"ub\": 10,\n",
    "                \"step\": 1\n",
    "            },\n",
    "            \"include_repeated_phrases\": {\n",
    "                \"_type\": \"discrete\",\n",
    "                \"values\": [false, true]\n",
    "            }\n",
    "        },\n",
    "        \"yake\": {\n",
    "            \"ws\": {\n",
    "                \"_type\": \"range\",\n",
    "                \"lb\": 1,\n",
    "                \"ub\": 3,\n",
    "                \"step\": 1\n",
    "            },\n",
    "            \"dedup\": {\n",
    "                \"_type\": \"discrete\",\n",
    "                \"values\": [0.8, 0.9, 0.95]\n",
    "            },\n",
    "            \"dedup_method\": {\n",
    "                \"_type\": \"discrete\",\n",
    "                \"values\": [\"leve\", \"seqm\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "```\n",
    " <details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc2abe",
   "metadata": {},
   "source": [
    "##### Section 3: Define filepaths to training data, etc\n",
    "\n",
    "In `training` include: \n",
    "- `directory`: Base directory containing training data\n",
    "- `input_files`: global pattern for input text files\n",
    "- `keywords`: CSV file containing training keywords\n",
    "- `epsilon`: Learning rate or noise parameter for training\n",
    "- `save_file`: Output file for trained model or hyperparameters\n",
    "\n",
    "\n",
    "<details> <summary>Example</summary>\n",
    "  \n",
    "``` json\n",
    "\"training\": {\n",
    "        \"directory\": \"../private_data/slac_logs\",\n",
    "        \"input_files\": [\"*.txt\"],\n",
    "        \"keywords\": [\"slac_keywords.csv\"],\n",
    "        \"epsilon\": 0.05,\n",
    "        \"save_file\": \"slac_hyper.pkl\"\n",
    "    },\n",
    "```\n",
    " <details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef131f2",
   "metadata": {},
   "source": [
    "##### Section 4: Define filepaths to saving filwpaths\n",
    "\n",
    "In `saving` include: \n",
    "- `ouput_files_directory`: Output file directory for search results\n",
    "- `css_filepath`: Filepath to styling for highlighted text HTML\n",
    "\n",
    "<details> <summary>Example</summary>\n",
    "  \n",
    "``` json\n",
    "\"saving\": {\n",
    "        \"css_filepath\": \"../../shared/keyword_vis.css\",\n",
    "        \"output_files_directory\": \"../private_data/results\"\n",
    "    }\n",
    "```\n",
    " <details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236dc56",
   "metadata": {},
   "source": [
    "## Train and run models\n",
    "In this example, we pick the 'best' result for each algorithm by training on two files with some user-provided keywords.\n",
    "Then we extract keywords from a third file using the trained model.\n",
    "\n",
    "Using a searcher which will read in training data from a search configuration, select the best model's keywords. \n",
    "We save the results of the hyperparameter training in a serialize Python \"pickle\" file so we don't need to repeat the training.\n",
    "We could run the same hyperparameters on multiple files without retraining with `run_hyper()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b5e49d",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 90 column 5 (char 2267)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m config_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslac_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# TODO: If you would like to re-train the model, delete `private_data/slac_logs/slac_hyper.pkl`\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a Searcher object from the configuration \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m slac_searcher \u001b[38;5;241m=\u001b[39m \u001b[43mSearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_fp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/metadata-project/sufi/sciencesearch/sciencesearch/nlp/search.py:79\u001b[0m, in \u001b[0;36mSearcher.from_config\u001b[0;34m(cls, config_file)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_config\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config_file) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearcher\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     conf \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     training \u001b[38;5;241m=\u001b[39m conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     81\u001b[0m     file_dir \u001b[38;5;241m=\u001b[39m Path(training[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 90 column 5 (char 2267)"
     ]
    }
   ],
   "source": [
    "# TODO: Enter filepath to your configuration\n",
    "config_fp = \"slac_config.json\"\n",
    "\n",
    "# TODO: If you would like to re-train the model, delete `private_data/slac_logs/slac_hyper.pkl`\n",
    "\n",
    "# Create a Searcher object from the configuration \n",
    "slac_searcher = Searcher.from_config(config_file=config_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6ab34",
   "metadata": {},
   "source": [
    "### With Searcher object, search for all files that have a certain keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98408751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'slac_log_test.txt'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all files that have a keyword\n",
    "keyword = \"diffraction\"\n",
    "slac_searcher.find(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928712c0",
   "metadata": {},
   "source": [
    "### Create JsonVisualizer object with Searcher object\n",
    "\n",
    "With JsonVisualizer, you can \n",
    "1. Save predicted, or training keywords as a json file\n",
    "2. Create a text HTML file with keywords highlighted in the context of the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "slac_json_viewer = JsonView(searcher=slac_searcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ddfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slac_log_test.txt => diluted, ludicrous, jet, transmission, xtals, sample, shooting, hit, protein, diffraction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'slac_log_test.txt': ['diluted',\n",
       "  'ludicrous',\n",
       "  'jet',\n",
       "  'transmission',\n",
       "  'xtals',\n",
       "  'sample',\n",
       "  'shooting',\n",
       "  'hit',\n",
       "  'protein',\n",
       "  'diffraction']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slac_json_viewer.training_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318187a",
   "metadata": {},
   "source": [
    "#### Save and visualize a single set of keywords\n",
    "\n",
    "In this example, predicted keywords are saved, and the resulting saved json is visualized as a html page per input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfe4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath: ../private_data/slac_logs/slac_log_test.txt\n"
     ]
    }
   ],
   "source": [
    "# save keywords\n",
    "slac_json_viewer.save_predicted_keywords(filename = '../private_data/results/predicted_keywords.json')\n",
    "\n",
    "# open HTML to see keywords in context of the file's text\n",
    "JsonView.visualize_from_config(config_file=config_fp, json_file=\"predicted_keywords.json\", save_filename=\"predicted_keywords_html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb2eb5",
   "metadata": {},
   "source": [
    "#### Save and visualize multiple sets of keywords\n",
    "\n",
    "In this example, predicted keywords and training keywords are saved, and the resulting saved json is visualized as a html page per input file, color coded to differentiate each set of keywords with a key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec8ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath: ../private_data/slac_logs/slac_log_test.txt\n"
     ]
    }
   ],
   "source": [
    "slac_json_viewer.save_all_keyword_sets('../private_data/results/keywords_all_sets.json')\n",
    "JsonView.visualize_from_config(config_file=config_fp,json_file=\"keywords_all_sets.json\", save_filename=\"keywords_all_sets_html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
