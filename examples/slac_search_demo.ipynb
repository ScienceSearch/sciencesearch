{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57515761",
   "metadata": {},
   "source": [
    "## __Notebook to Reproduce Results and Run Experiments__\n",
    "\n",
    "### Steps for reproducing results  \n",
    "- Step 0: Import modules\n",
    "\n",
    "- Step 1: Setup demo with database\n",
    "\n",
    "- Step 2: Select which experiment you would like to run\n",
    "\n",
    "- Step 3: Preprocess files \n",
    "\n",
    "- Step 4: Train and run models\n",
    "\n",
    "- Step 5: Visualize results in context of the input file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9b40f",
   "metadata": {},
   "source": [
    "### Step 0: Import modules\n",
    "Import modules and set up logging\n",
    "See [documentation](https://deepwiki.com/ScienceSearch/sciencesearch/1-overview) for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808a9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "from sciencesearch.nlp.hyper import Hyper, algorithms_from_results\n",
    "from sciencesearch.nlp.sweep import Sweep\n",
    "from sciencesearch.nlp.models import Rake, Yake, KPMiner, Ensemble\n",
    "from sciencesearch.nlp.train import train_hyper, load_hyper, run_hyper\n",
    "from sciencesearch.nlp.search import KeywordExplorer\n",
    "from operator import attrgetter\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "\n",
    "logging.root.setLevel(logging.ERROR)  # silence pke warnings\n",
    "slog = logging.getLogger(\"sciencesearch\")\n",
    "slog.setLevel(logging.WARNING)\n",
    "from sciencesearch.nlp.visualize_kws import JsonView\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e342621",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831dfdc3",
   "metadata": {},
   "source": [
    "### Step 1: Setup demo with database\n",
    "*This demo will only work if you are a SLAC employee with access to the correct data.*\n",
    "\n",
    "***\n",
    "\n",
    "To begin, please create a `private_data` folder in the root directory `sciencesearch/` \n",
    "\n",
    "In the `private_data` folder add a database source\n",
    "\n",
    "\n",
    "*Note: This code is generalizable to any database with  `logbook` and `experiments` tables (See structure in `simplified_elog.db`)*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c58ff5",
   "metadata": {},
   "source": [
    "### Step 2: Select a source to extract keywords\n",
    "\n",
    "Extract keywords from:\n",
    "\n",
    "1. All experiment logs (elogs)\n",
    "2. Only experiment descriptions\n",
    "3. Only elogs with experiment parameters\n",
    "4. Only elogs that are misc. commentary\n",
    "\n",
    "See README.md for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae11f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config_files/slac_config_params.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Select an experiment to run\n",
    "source = 4\n",
    "\n",
    "source_configs = {\n",
    "    1: \"slac_config_all_elogs.json\",\n",
    "    2: \"slac_config_descriptions.json\",\n",
    "    3: \"slac_config_params.json\",\n",
    "    4: \"slac_config_commentary.json\"\n",
    "}\n",
    "\n",
    "\n",
    "# Set config filepath \n",
    "config_fp = f'config_files/{source_configs[source]}'\n",
    "config_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543705dd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b573a17",
   "metadata": {},
   "source": [
    "### Step 3: Set up config and preprocess files \n",
    "\n",
    "The filepath to your database should be specified in the configuration\n",
    "\n",
    "```\n",
    "\"database\": \"private_data/{database_name}.db\"\n",
    "```\n",
    "\n",
    "\n",
    "Preprocessed files will be saved as `[.txt]` files in the directory specified in the config.\n",
    "\n",
    "This need not be updated.\n",
    "\n",
    "```\n",
    "\"training\": {\n",
    "        \"directory\": \"../private_data/{custom_directory}\",\n",
    "        \"input_files\": [\"*.txt\"],\n",
    "}\n",
    "```\n",
    "#### Run preprocessing of data files for the selected experiemnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4fc5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOWER(TRIM(content)) LIKE 'running%' OR LOWER(TRIM(content)) LIKE  'beam on%' OR LOWER(TRIM(content)) LIKE  'run number%' OR LOWER(TRIM(content)) LIKE  'sample name%' OR LOWER(TRIM(content)) LIKE  'RE(bp.daq_%' OR LOWER(TRIM(content)) LIKE  'notch scan%' OR LOWER(TRIM(content)) LIKE  'lxt scan%'\n",
      "CREATE TABLE logbook_parameters AS\n",
      "        SELECT * FROM logbook_reduced\n",
      "        WHERE tags IN ('SQ1', 'daq_scan', 'TMO_KB01', 'fs_timing', 'hv', 'IM1K4', 'laser_trigger', 'kbo1,presets', 'sl1k4,update_preset',  'sl2k0,update_preset', 'sq1,presets', 'li2k4,PMPS_toggle',  'TIXEL', 'sq1_z_scan_ion', 'sq1_z_scan', 'DARK', 'SQ1,position', 'SQ1,XY', 'GoldenTrajectory,IM5K4', 'laser', 'sq1','z,scan', 'sl1k4','ppm_data','NNO', 'high,transmission,high,count','filamentation'\n",
      "        )\n",
      "        OR LOWER(TRIM(content)) LIKE 'running%' OR LOWER(TRIM(content)) LIKE  'beam on%' OR LOWER(TRIM(content)) LIKE  'run number%' OR LOWER(TRIM(content)) LIKE  'sample name%' OR LOWER(TRIM(content)) LIKE  'RE(bp.daq_%' OR LOWER(TRIM(content)) LIKE  'notch scan%' OR LOWER(TRIM(content)) LIKE  'lxt scan%'\n",
      "        ;\n"
     ]
    }
   ],
   "source": [
    "from sciencesearch.nlp.slac_data_extractor import SLACDatabaseDataExtractor\n",
    "\n",
    "# Setup DBDataExtractor\n",
    "data_extractor = SLACDatabaseDataExtractor(config_file=config_fp)\n",
    "data_extractor.get_tables()\n",
    "# Run the corresponding data extraction and cleaning methods based on your experiment type\n",
    "if source == 1:\n",
    "    \"\"\" Preprocess files for experiment 1\n",
    "    results will be saved in private_data/slac_logs\"\"\"\n",
    "    data_extractor.process_elogs()\n",
    "\n",
    "elif source == 2:\n",
    "    \"\"\" Preprocess files for experiment 2\n",
    "    results will be saved in private_data/descriptions \"\"\"\n",
    "    data_extractor.process_experiment_descriptions()\n",
    "\n",
    "elif source == 3:\n",
    "    \"\"\" Preprocess files for experiment 3\n",
    "    results will be saved in private_data/params \"\"\"\n",
    "    data_extractor.process_experiment_elog_parameters()\n",
    "\n",
    "elif source == 4:\n",
    "    \"\"\" Preprocess files for experiment 4\n",
    "    results will be saved in private_data/commentary \"\"\"\n",
    "    data_extractor.process_experiment_elog_commentary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cdd4d8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4cc2f",
   "metadata": {},
   "source": [
    "### Step 4: Train and run models\n",
    "\n",
    "When a KeywordExtractor object is created with a configuration file it:\n",
    "\n",
    "1. Reads in training data from a search configuration\n",
    "\n",
    "2. Hyperparameter optimization: compares many algorithm parameters to select the highest performing algorithm settings\n",
    "\n",
    "3. Trains models according to the 'best' hyperparameters \n",
    "\n",
    "4. Extracts keywords using the trained models\n",
    "\n",
    "\n",
    "We save the models in a serialized Python \"pickle\" file so we don't need to repeat the training.\n",
    "\n",
    "We could use the same models on multiple files without retraining with `run_hyper()`\n",
    "\n",
    "*Note: if you would like to re-train the model, delete `private_data/{training_directory}/{save_file}.pkl*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27942a6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a KeywordExplorer object from the configuration\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m slac_searcher \u001b[38;5;241m=\u001b[39m \u001b[43mKeywordExplorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_fp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/metadata-project/sufi/sciencesearch/sciencesearch/nlp/search.py:153\u001b[0m, in \u001b[0;36mKeywordExplorer.from_config\u001b[0;34m(cls, config_file)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_file_pat \u001b[38;5;129;01min\u001b[39;00m training[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_files\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m glob(input_file_pat, root_dir\u001b[38;5;241m=\u001b[39mfile_dir):\n\u001b[0;32m--> 153\u001b[0m         kw \u001b[38;5;241m=\u001b[39m \u001b[43mrun_hyper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyper_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         search_kw[fname] \u001b[38;5;241m=\u001b[39m kw\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# initialize this class with results\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/metadata-project/sufi/sciencesearch/sciencesearch/nlp/train.py:71\u001b[0m, in \u001b[0;36mrun_hyper\u001b[0;34m(hyper_results, text_file, text, num_keywords)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_file\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mensm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/metadata-project/sufi/sciencesearch/sciencesearch/nlp/models.py:213\u001b[0m, in \u001b[0;36mAlgorithm.run\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    211\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stem_text(text)\n\u001b[1;32m    212\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m--> 213\u001b[0m kw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t1\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_timings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstem\u001b[39m\u001b[38;5;124m\"\u001b[39m: t1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextract\u001b[39m\u001b[38;5;124m\"\u001b[39m: t2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m: t1 \u001b[38;5;241m+\u001b[39m t2}\n",
      "File \u001b[0;32m~/Documents/metadata-project/sufi/sciencesearch/sciencesearch/nlp/models.py:557\u001b[0m, in \u001b[0;36mEnsemble._get_keywords\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    555\u001b[0m scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_algorithms:\n\u001b[0;32m--> 557\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m \u001b[43malg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m keywords:\n\u001b[1;32m    559\u001b[0m         merged_kw\u001b[38;5;241m.\u001b[39madd(kw)\n",
      "File \u001b[0;32m~/Documents/metadata-project/sufi/sciencesearch/sciencesearch/nlp/models.py:213\u001b[0m, in \u001b[0;36mAlgorithm.run\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    211\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stem_text(text)\n\u001b[1;32m    212\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m--> 213\u001b[0m kw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t1\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_timings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstem\u001b[39m\u001b[38;5;124m\"\u001b[39m: t1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextract\u001b[39m\u001b[38;5;124m\"\u001b[39m: t2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m: t1 \u001b[38;5;241m+\u001b[39m t2}\n",
      "File \u001b[0;32m~/Documents/metadata-project/sufi/sciencesearch/sciencesearch/nlp/models.py:411\u001b[0m, in \u001b[0;36mKPMiner._get_keywords\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_keywords\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    410\u001b[0m     stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mstopwords\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_document\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstoplist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extractor\u001b[38;5;241m.\u001b[39mcandidate_selection(\n\u001b[1;32m    413\u001b[0m         lasf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mlasf, cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mcutoff\n\u001b[1;32m    414\u001b[0m     )\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extractor\u001b[38;5;241m.\u001b[39mcandidate_weighting(\n\u001b[1;32m    416\u001b[0m         df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mdoc_freq_info,\n\u001b[1;32m    417\u001b[0m         sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39msigma,\n\u001b[1;32m    418\u001b[0m         alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39malpha,\n\u001b[1;32m    419\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/pke/base.py:98\u001b[0m, in \u001b[0;36mLoadFile.load_document\u001b[0;34m(self, input, language, stoplist, normalization, spacy_model)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     97\u001b[0m     parser \u001b[38;5;241m=\u001b[39m RawTextReader(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage)\n\u001b[0;32m---> 98\u001b[0m     sents \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacy_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspacy_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# check whether input is processed text\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/pke/readers.py:85\u001b[0m, in \u001b[0;36mRawTextReader.read\u001b[0;34m(self, text, spacy_model)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# select first model for the language\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(installed_models):\n\u001b[0;32m---> 85\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstalled_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtextcat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# stop execution is no model is available\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     excp_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo downloaded spacy model for \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m language.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/util.py:465\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/util.py:501\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/en_core_web_sm/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/util.py:547\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    538\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[1;32m    539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[1;32m    540\u001b[0m     config,\n\u001b[1;32m    541\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    546\u001b[0m )\n\u001b[0;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/language.py:2246\u001b[0m, in \u001b[0;36mLanguage.from_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;66;03m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[1;32m   2245\u001b[0m     exclude \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(exclude) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 2246\u001b[0m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m path  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   2248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_link_components()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/util.py:1390\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, reader \u001b[38;5;129;01min\u001b[39;00m readers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1388\u001b[0m     \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n\u001b[0;32m-> 1390\u001b[0m         \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/language.py:2232\u001b[0m, in \u001b[0;36mLanguage.from_disk.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m   2230\u001b[0m deserializers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta.json\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m deserialize_meta  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m deserializers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m deserialize_vocab  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 2232\u001b[0m deserializers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[union-attr]\u001b[39;49;00m\n\u001b[1;32m   2233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, proc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_components:\n\u001b[1;32m   2236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m exclude:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/tokenizer.pyx:787\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.from_disk\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/tokenizer.pyx:855\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.from_bytes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/tokenizer.pyx:131\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.rules.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/tokenizer.pyx:580\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._load_special_cases\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/tokenizer.pyx:624\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/tokenizer.pyx:174\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/tokens/doc.pyx:254\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/scisearch/lib/python3.10/site-packages/spacy/tokens/_dict_proxies.py:26\u001b[0m, in \u001b[0;36mSpanGroups.__init__\u001b[0;34m(self, doc, items)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A dict-like proxy held by the Doc, to control access to span groups.\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m _EMPTY_BYTES \u001b[38;5;241m=\u001b[39m srsly\u001b[38;5;241m.\u001b[39mmsgpack_dumps([])\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m, doc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m, items: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, SpanGroup]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m     28\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_ref \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(doc)\n\u001b[1;32m     30\u001b[0m     UserDict\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, items)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a KeywordExplorer object from the configuration\n",
    "slac_searcher = KeywordExplorer.from_config(config_file=config_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f39fd5",
   "metadata": {},
   "source": [
    "#### Explore keyword results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### See all file keywords (predicted and training)\n",
    "# slac_searcher.file_keywords\n",
    "\n",
    "### See all predicted keywords\n",
    "# predicted_keywords = slac_searcher.predicted_keywords\n",
    "\n",
    "### See training keywords\n",
    "# slac_searcher.training_keywords\n",
    "\n",
    "# See both training keywords and predicted keywords\n",
    "slac_searcher.training_and_predicted_keywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f4a74",
   "metadata": {},
   "source": [
    "#### Search for all experiments that have a particular keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d75198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set keyword variable to a keyword you would like to look for\n",
    "\n",
    "keyword = \"magnet\"\n",
    "slac_searcher.find(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e7192",
   "metadata": {},
   "source": [
    "#### Save keywords\n",
    "Location for results is defined in the configuration file. \n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "\"saving\": {\n",
    "        \"css_filepath\": \"../../shared/keyword_vis.css\",\n",
    "        \"output_files_directory\": \"../private_data/results\"\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define save file name\n",
    "file_name = f\"experiment_{source}_results\"\n",
    "slac_searcher.save_keywords_to_file(file_name = file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affad275",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2b46e",
   "metadata": {},
   "source": [
    "### Step 5: Visualize keywords\n",
    "\n",
    "Highlight the keywords in the input file (HTML)\n",
    "\n",
    "Options include \n",
    "\n",
    "(1) View training and/or predicted keywords\n",
    "\n",
    "(2) Seeing one or all files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914a059",
   "metadata": {},
   "source": [
    "#### Visualize keywords: Single file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b231d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: set file name to {experiment_id}.txt\n",
    "filename = \"test.txt\"\n",
    "HTML(\n",
    "    slac_searcher.view_keywords(\n",
    "        show_training=True, show_predicted=True, textfilename=filename\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca5fed",
   "metadata": {},
   "source": [
    "#### Visualize keywords: All files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "# View keywords in context of text logs (all files)\n",
    "HTML(\n",
    "    slac_searcher.view_keywords(\n",
    "        show_training=True, show_predicted=True, textfilename=None\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
