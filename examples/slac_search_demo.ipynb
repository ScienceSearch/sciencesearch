{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57515761",
   "metadata": {},
   "source": [
    "# Extract keywords from SLAC experiment logs\n",
    "\n",
    "This example notebook will demonstrate how to configure and run the ScienceSearch Python tools for keyword extraction.\n",
    "\n",
    "For more information about ScienceSearch, see also:\n",
    "- [sciencesearch Github repository](https://github.com/ScienceSearch/sciencesearch).\n",
    "- AI-generated [documentation pages](https://deepwiki.com/ScienceSearch/sciencesearch/1-overview).\n",
    "\n",
    "## Prerequisites\n",
    "- A Python environment which includes ScienceSearch Python package `sciencesearch` (see [../README.md](../README.md))\n",
    "- A SLAC-generated SQLite database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9b40f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Python imports and some logging setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "from sciencesearch.nlp.search import KeywordExplorer\n",
    "from sciencesearch.nlp.slac_data_extractor import SLACDatabaseDataExtractor\n",
    "from IPython.core.display import HTML\n",
    "from sciencesearch.nlp.visualize_kws import JsonView\n",
    "from pathlib import Path\n",
    "import json\n",
    "import zipfile as zf\n",
    "import os\n",
    "\n",
    "# logging setup\n",
    "import logging\n",
    "\n",
    "logging.root.setLevel(logging.ERROR)  # silence pke warnings\n",
    "slog = logging.getLogger(\"sciencesearch\")\n",
    "slog.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831dfdc3",
   "metadata": {},
   "source": [
    "## Initialize source database\n",
    "Before you can run the algorithms, you need to copy your SLAC-generated database into a file called \"simplified_elog.db\" in the \"private_data\" directory.\n",
    "\n",
    "The database must have the _logbook_ and _experiments_ tables.\n",
    "\n",
    "You will also need a file called \"queries_info.json\" in the private data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae0b3f-3327-4290-85ed-84c1be7a9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create private data directory\n",
    "\n",
    "# use starter folder\n",
    "p_zip = Path(\n",
    "    \"../private_data_starter.zip\"\n",
    ")  # assume this notebook is run from the `examples/` subdirectory\n",
    "\n",
    "p_folder = Path(\n",
    "    \"../private_data\"\n",
    ")  # assume this notebook is run from the `examples/` subdirectory\n",
    "\n",
    "# Unzip example private data if it exists and existing private data folder does not exist\n",
    "if not p_folder.exists() and p_zip.exists():\n",
    "    with zf.ZipFile(\"../private_data_starter.zip\", \"r\") as files:\n",
    "        for file in files.namelist():\n",
    "            if not file.startswith(\"__MACOSX\"):\n",
    "                files.extract(file, \"../\")\n",
    "    print(\"Starter private data folder is unzipped and ready\")\n",
    "# Create private_data folder from scratch if starter does not exist\n",
    "# Check if private data exists\n",
    "elif not p_zip.exists() and not p_folder.exists():\n",
    "    p_folder.mkdir(exist_ok=True)\n",
    "    dbfile = \"simplified_elog.db\"\n",
    "    if not (p_folder / dbfile).exists():\n",
    "        print(f\"Please copy database to:\\n{p_folder.resolve() / dbfile}\")\n",
    "    if not (p_folder / \"queries_info.json\").exists():\n",
    "        print(f\"Please copy 'queries_info.json' to directory {p_folder.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdb086",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c51a4a0",
   "metadata": {},
   "source": [
    "## Initialize configuration file\n",
    "You also will need a configuration file specifying the algorithms, initial settings, and directory locations.\n",
    "For the initial run, which uses all the elogs, this file will be in the \"config_files\" directory and be named \"slac_config_all_elogs.json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b48edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dir = Path(\".\") / \"config_files\"\n",
    "conf_dir.mkdir(exist_ok=True)\n",
    "conf_file = conf_dir / \"slac_config_all_elogs.json\"\n",
    "if not conf_file.exists():\n",
    "    print(f\"Please create configuration file {conf_file.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db378baf",
   "metadata": {},
   "source": [
    "## Populate training data \n",
    "Before you can train the models, you will need to provide training data.\n",
    "\n",
    "Format: \n",
    "```\n",
    "filename1, \"list,of,keywords\"\n",
    "filename2, \"another,list,of,keywords\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find directory to place training data\n",
    "conf = json.load(open(conf_file))\n",
    "input_file_dir = Path(conf[\"training\"][\"directory\"])\n",
    "training_keywords_file = input_file_dir / conf[\"training\"][\"keywords\"][0]\n",
    "if not conf_file.exists():\n",
    "    print(f\"Please add training data file {training_keywords_file.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123f706-c6ad-479c-93b6-2519f642d2bf",
   "metadata": {},
   "source": [
    "## Extract keywords from elogs\n",
    "Using the provided configuration file, we will tell ScienceSearch to perform the following steps:\n",
    "1. Load data from the database using the `SlacDatabaseDataExtractor` class\n",
    "2. Call the appropriate method on this class to preprocess the data to remove non-technical words, HTML tags, etc.\n",
    "3. Using the `KeywordExplorer` class, choose the 'best' keyword extraction based on a comparison with training data and extract keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6e825-623d-4c39-ab38-f4f1bf4695eb",
   "metadata": {},
   "source": [
    "## Extract keywords from other sources\n",
    "In addition to elogs, we have written some variations of the process above to extract from:\n",
    "* experiment descriptions\n",
    "* elogs and experiment parameters\n",
    "* elogs that are labeled as misc. commentary\n",
    "\n",
    "These variations are coded into methods in the `SLACDatabaseDataExtractor` class. Distinct configuration files are used to keep the hyperparameters and output data cleanly separated.\n",
    "\n",
    "Uncomment the appropriate line below to run one of these other experiments.\n",
    "\n",
    "You will also need to make sure the corresponding directory and configuration file are created for these to run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68720a95-ac98-49a2-bffb-2d3c0418fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data preprocessing class\n",
    "#     option replace_abbrv (bool) if you would like to expand acronyms\n",
    "#     defaults to False\n",
    "data_extractor = SLACDatabaseDataExtractor(conf_file, replace_abbrv=False)\n",
    "# load and preprocess data\n",
    "print(\"Load and preprocess data\")\n",
    "data_extractor.process_elogs()\n",
    "# choose keyword parameters and extract keywords\n",
    "print(\"Extracting keywords - this may take a minute or two\")\n",
    "kwe = KeywordExplorer.from_config(conf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e04a4-0d92-467d-a19f-ffba62a6a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show file keywords\n",
    "print(\"\\n\".join([f\"{k} => {', '.join(v)}\" for k, v in kwe.file_keywords.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb24b20-3a66-4f0a-b6b6-484534999807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To try another set of inputs, set the value accordingly\n",
    "which = -1  # set to 1, 2, or 3\n",
    "\n",
    "if which == 1:\n",
    "    # Experiment descriptions\n",
    "    conf_file = conf_dir / \"slac_config_descriptions.json\"\n",
    "    SLACDatabaseDataExtractor(\n",
    "        conf_file, replace_abbrv=False\n",
    "    ).process_experiment_descriptions()\n",
    "elif which == 2:\n",
    "    # Elogs and experiment parameters\n",
    "    conf_file = conf_dir / \"slac_config_params.json\"\n",
    "    SLACDatabaseDataExtractor(\n",
    "        conf_file, replace_abbrv=False\n",
    "    ).process_experiment_descriptions()\n",
    "elif which == 3:\n",
    "    # Only elogs that are misc. commentary\n",
    "    conf_file = conf_dir / \"slac_config_commentary.json\"\n",
    "    SLACDatabaseDataExtractor(\n",
    "        conf_file, replace_abbrv=False\n",
    "    ).process_experiment_descriptions()\n",
    "\n",
    "if which > 0:\n",
    "    # Common: extract keywords with chosen algorithm\n",
    "    kwe = KeywordExplorer.from_config(conf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f39fd5",
   "metadata": {},
   "source": [
    "## Explore keyword results\n",
    "We can now use the extracted keywords together with the original text to either search or visualize the keywords in context.\n",
    "The code below uses the `KeywordExplorer` instance created when you extracted the keywords in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff635f52-eb47-4e6b-a62b-7e24fa5af06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predicted keywords as a graph (created with Graphviz)\n",
    "kwe.export(format=\"graph\")  # format options: \"graph\", \"excel\", \"json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training and predicted keywords\n",
    "kwe.training_and_predicted_keywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f4a74",
   "metadata": {},
   "source": [
    "### Search for all experiments that have a particular keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d75198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a keyword\n",
    "keyword = \"heme\"\n",
    "kwe.find(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914a059",
   "metadata": {},
   "source": [
    "### Visualize keywords\n",
    "You can also view the keywords in context with a styled HTML output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b231d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"{change-file-here}.txt\"\n",
    "filename = None  # all files\n",
    "HTML(kwe.view_keywords(show_training=True, show_predicted=True, textfilename=filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
