{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57515761",
   "metadata": {},
   "source": [
    "# Extract keywords from SLAC experiment logs\n",
    "\n",
    "This example notebook will demonstrate how to configure and run the ScienceSearch Python tools for keyword extraction.\n",
    "\n",
    "For more information about ScienceSearch, see also:\n",
    "- [sciencesearch Github repository](https://github.com/ScienceSearch/sciencesearch).\n",
    "- AI-generated [documentation pages](https://deepwiki.com/ScienceSearch/sciencesearch/1-overview).\n",
    "\n",
    "## Prerequisites\n",
    "- A Python environment which includes ScienceSearch Python package `sciencesearch` (see [../README.md](../README.md))\n",
    "- A SLAC-generated SQLite database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9b40f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Python imports and some logging setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "from sciencesearch.nlp.search import KeywordExplorer\n",
    "from sciencesearch.nlp.slac_data_extractor import SLACDatabaseDataExtractor\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# logging setup\n",
    "import logging\n",
    "\n",
    "logging.root.setLevel(logging.ERROR)  # silence pke warnings\n",
    "slog = logging.getLogger(\"sciencesearch\")\n",
    "slog.setLevel(logging.WARNING)\n",
    "from sciencesearch.nlp.visualize_kws import JsonView\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831dfdc3",
   "metadata": {},
   "source": [
    "## Initialize source database\n",
    "Before you can run the algorithms, you need to copy your SLAC-generated database into a file called \"simplified_elog.db\" in the \"private_data\" directory.\n",
    "\n",
    "The database must have the _logbook_ and _experiments_ tables.\n",
    "\n",
    "You will also need a file called \"queries_info.json\" in the private data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae0b3f-3327-4290-85ed-84c1be7a9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target directory\n",
    "p = Path(\"../private_data\") # assume this notebook is run from the `examples/` subdirectory\n",
    "p.mkdir(exist_ok=True)\n",
    "dbfile = 'simplified_elog.db'\n",
    "if not (p / dbfile).exists():\n",
    "    print(f\"Please copy database to:\\n{p.resolve() / dbfile}\")\n",
    "if not (p / \"queries_info.json\").exists():\n",
    "    print(f\"Please copy 'queries_info.json' to directory {p.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51a4a0",
   "metadata": {},
   "source": [
    "## Initialize configuration file\n",
    "You also will need a configuration file specifying the algorithms, initial settings, and directory locations.\n",
    "For the initial run, which uses all the elogs, this file will be in the \"config_files\" directory and be named \"slac_config_all_elogs.json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b48edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dir = Path(\".\") / \"config_files\"\n",
    "conf_dir.mkdir(exist_ok=True)\n",
    "conf_file = conf_dir / \"slac_config_all_elogs.json\"\n",
    "if not conf_file.exists():\n",
    "    print(f\"Please create configuration file {conf_file.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db378baf",
   "metadata": {},
   "source": [
    "## Populate training data \n",
    "Before you can train the models, you will need to provide training data.\n",
    "\n",
    "Format: \n",
    "```\n",
    "filename1, \"list,of,keywords\"\n",
    "filename2, \"another,list,of,keywords\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find directory to place training data\n",
    "conf = json.load(open(conf_file))\n",
    "input_file_dir = Path(conf[\"training\"][\"directory\"])\n",
    "training_keywords_file = input_file_dir / conf[\"training\"][\"keywords\"][0]\n",
    "if not conf_file.exists():\n",
    "    print(f\"Please add training data file {training_keywords_file.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123f706-c6ad-479c-93b6-2519f642d2bf",
   "metadata": {},
   "source": [
    "## Extract keywords from elogs\n",
    "Using the provided configuration file, we will tell ScienceSearch to perform the following steps:\n",
    "1. Load data from the database using the `SlacDatabaseDataExtractor` class\n",
    "2. Call the appropriate method on this class to preprocess the data to remove non-technical words, HTML tags, etc.\n",
    "3. Using the `KeywordExplorer` class, choose the 'best' keyword extraction based on a comparison with training data and extract keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6e825-623d-4c39-ab38-f4f1bf4695eb",
   "metadata": {},
   "source": [
    "## Extract keywords from other sources\n",
    "In addition to elogs, we have written some variations of the process above to extract from:\n",
    "* experiment descriptions\n",
    "* elogs and experiment parameters\n",
    "* elogs that are labeled as misc. commentary\n",
    "\n",
    "These variations are coded into methods in the `SLACDatabaseDataExtractor` class. Distinct configuration files are used to keep the hyperparameters and output data cleanly separated.\n",
    "\n",
    "Uncomment the appropriate line below to run one of these other experiments.\n",
    "\n",
    "You will also need to make sure the corresponding directory and configuration file are created for these to run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68720a95-ac98-49a2-bffb-2d3c0418fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data preprocessing class\n",
    "data_extractor = SLACDatabaseDataExtractor(conf_file)\n",
    "# load and preprocess data\n",
    "print(\"Load and preprocess data\")\n",
    "data_extractor.process_elogs()\n",
    "# choose keyword parameters and extract keywords\n",
    "print(\"Extracting keywords - this may take a minute or two\")\n",
    "kwe = KeywordExplorer.from_config(conf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e04a4-0d92-467d-a19f-ffba62a6a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show file keywords\n",
    "print(\"\\n\".join([f\"{k} => {', '.join(v)}\" for k,v in kwe.file_keywords.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment ONE of the following sections\n",
    "\n",
    "## Experiment descriptions\n",
    "# conf_file =  conf_dir / \"slac_config_descriptions.json\"\n",
    "# SLACDatabaseDataExtractor(conf_file).process_experiment_descriptions()\n",
    "\n",
    "## Elogs and experiment parameters\n",
    "conf_file =  conf_dir / \"slac_config_params.json\"\n",
    "SLACDatabaseDataExtractor(conf_file).process_experiment_descriptions()\n",
    "\n",
    "## Only elogs that are misc. commentary\n",
    "# conf_file =  conf_dir / \"slac_config_commentary.json\"\n",
    "# SLACDatabaseDataExtractor(conf_file)d.process_experiment_descriptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180b14b-131d-4926-bf25-afc09eb24cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common: extract keywords with chosen algorithm\n",
    "kwe = KeywordExplorer.from_config(conf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f39fd5",
   "metadata": {},
   "source": [
    "## Explore keyword results\n",
    "We can now use the extracted keywords together with the original text to either search or visualize the keywords in context.\n",
    "The code below uses the `KeywordExplorer` instance created when you extracted the keywords in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training and predicted keywords\n",
    "kwe.training_and_predicted_keywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f4a74",
   "metadata": {},
   "source": [
    "### Search for all experiments that have a particular keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d75198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a keyword\n",
    "keyword = \"magnet\"\n",
    "kwe.find(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914a059",
   "metadata": {},
   "source": [
    "### Visualize keywords\n",
    "You can also view the keywords in context with a styled HTML output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b231d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"mfxp17218_content.txt\"\n",
    "# filename = None  # all files\n",
    "HTML(kwe.view_keywords(\n",
    "        show_training=True, show_predicted=True, textfilename=filename\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
